{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3139,"status":"ok","timestamp":1712902765059,"user":{"displayName":"jingjing sun","userId":"10008707095167667488"},"user_tz":-480},"id":"SkDurgmseVvS","outputId":"d61a66d6-9f64-4aa1-e59a-ba1c4069c463"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1712902776160,"user":{"displayName":"jingjing sun","userId":"10008707095167667488"},"user_tz":-480},"id":"kaJA8bJJgY-O","outputId":"845fa71e-d11e-4821-aa5f-f7e710a582c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/cs5260/Bert-finetuning-with-Colossal-AI\n"]}],"source":["cd /content/drive/MyDrive/cs5260/Bert-finetuning-with-Colossal-AI/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5513,"status":"ok","timestamp":1712902785804,"user":{"displayName":"jingjing sun","userId":"10008707095167667488"},"user_tz":-480},"id":"8r0yFJeCfSwQ","outputId":"1709394e-f5f1-4f9a-da58-0d009808dd0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: colossalai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.6)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.18.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.66.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.38.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.2.2)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.7.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (24.0)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (1.11.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (0.4.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (2.6.4)\n","Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (2.10.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (0.1.99)\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai->-r requirements.txt (line 1)) (3.20.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (0.70.16)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (0.20.3)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 2)) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (3.13.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (3.9.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (6.0.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 4)) (12.4.127)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (0.15.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (3.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (2024.2.2)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai->-r requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai->-r requirements.txt (line 1)) (1.2.14)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai->-r requirements.txt (line 1)) (4.12.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai->-r requirements.txt (line 1)) (2.5.35)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai->-r requirements.txt (line 1)) (20.25.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai->-r requirements.txt (line 1)) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai->-r requirements.txt (line 1)) (2.16.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai->-r requirements.txt (line 1)) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai->-r requirements.txt (line 1)) (1.0.8)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai->-r requirements.txt (line 1)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai->-r requirements.txt (line 1)) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai->-r requirements.txt (line 1)) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai->-r requirements.txt (line 1)) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai->-r requirements.txt (line 1)) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai->-r requirements.txt (line 1)) (4.1.2)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai->-r requirements.txt (line 1)) (42.0.5)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai->-r requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai->-r requirements.txt (line 1)) (2.5)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai->-r requirements.txt (line 1)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai->-r requirements.txt (line 1)) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai->-r requirements.txt (line 1)) (0.18.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai->-r requirements.txt (line 1)) (2.22)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":513694,"status":"ok","timestamp":1712892945657,"user":{"displayName":"jingjing sun","userId":"10008707095167667488"},"user_tz":-480},"id":"FX3GLXXXN84g","outputId":"3c1c1553-645c-43dc-fe00-716ccb808bfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ BATCH_SIZE=64\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ echo 'Running with plugin: torch_ddp and batch size: 64'\n","Running with plugin: torch_ddp and batch size: 64\n","+ torchrun --standalone --nproc_per_node=1 finetune.py --target_f1 0.86 --plugin torch_ddp --batch_size 64\n","2024-04-12 03:27:16.356814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-12 03:27:16.356866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-12 03:27:16.358253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-12 03:27:17.467553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 03:27:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","Map: 100% 3668/3668 [00:00<00:00, 9375.29 examples/s]\n","Map: 100% 408/408 [00:00<00:00, 8634.09 examples/s]\n","Map: 100% 1725/1725 [00:00<00:00, 9448.36 examples/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09194684028625488 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10906529426574707 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","Epoch [1/3]:   0% 0/57 [00:01<?, ?it/s, loss=0.643]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 57/57 [01:09<00:00,  1.21s/it, loss=0.555]\n","Epoch [2/3]: 100% 57/57 [01:10<00:00,  1.24s/it, loss=0.339]\n","Epoch [3/3]: 100% 57/57 [01:10<00:00,  1.24s/it, loss=0.28]\n","{'accuracy': 0.8266666666666667, 'f1': 0.8748430305567183, 'loss': 0.41784197092056274}\n","+ '[' 0 -eq 0 ']'\n","+ echo 'Success: Plugin: torch_ddp with Batch Size: 64'\n","Success: Plugin: torch_ddp with Batch Size: 64\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ echo 'Running with plugin: torch_ddp_fp16 and batch size: 64'\n","Running with plugin: torch_ddp_fp16 and batch size: 64\n","+ torchrun --standalone --nproc_per_node=1 finetune.py --target_f1 0.86 --plugin torch_ddp_fp16 --batch_size 64\n","2024-04-12 03:31:23.305019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-12 03:31:23.305064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-12 03:31:23.306390: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-12 03:31:24.420307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 03:31:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","Map: 100% 408/408 [00:00<00:00, 7658.05 examples/s]\n","Map: 100% 1725/1725 [00:00<00:00, 9610.80 examples/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09258913993835449 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11069297790527344 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","Epoch [1/3]:   0% 0/57 [00:00<?, ?it/s, loss=0.643]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 57/57 [00:21<00:00,  2.60it/s, loss=0.555]\n","Epoch [2/3]: 100% 57/57 [00:22<00:00,  2.57it/s, loss=0.341]\n","Epoch [3/3]: 100% 57/57 [00:21<00:00,  2.61it/s, loss=0.275]\n","{'accuracy': 0.8272463768115942, 'f1': 0.8754180602006689, 'loss': 0.41678813099861145}\n","+ '[' 0 -eq 0 ']'\n","+ echo 'Success: Plugin: torch_ddp_fp16 with Batch Size: 64'\n","Success: Plugin: torch_ddp_fp16 with Batch Size: 64\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ echo 'Running with plugin: gemini and batch size: 64'\n","Running with plugin: gemini and batch size: 64\n","+ torchrun --standalone --nproc_per_node=1 finetune.py --target_f1 0.86 --plugin gemini --batch_size 64\n","2024-04-12 03:32:55.331140: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-12 03:32:55.331194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-12 03:32:55.332480: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-12 03:32:56.417286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 03:32:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","Parameter 'function'=<bound method GLUEDataBuilder.convert_to_features of <data.GLUEDataBuilder object at 0x791ed86c2980>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Map: 100% 3668/3668 [00:00<00:00, 9300.98 examples/s]\n","Map: 100% 408/408 [00:00<00:00, 8416.62 examples/s]\n","Map: 100% 1725/1725 [00:00<00:00, 9805.19 examples/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09820127487182617 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10702967643737793 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","Epoch [1/3]: 100% 57/57 [00:18<00:00,  3.04it/s, loss=0.554]\n","Epoch [2/3]: 100% 57/57 [00:18<00:00,  3.05it/s, loss=0.34]\n","Epoch [3/3]: 100% 57/57 [00:18<00:00,  3.01it/s, loss=0.28]\n","{'accuracy': 0.8231884057971014, 'f1': 0.8721174004192872, 'loss': 0.0}\n","+ '[' 0 -eq 0 ']'\n","+ echo 'Success: Plugin: gemini with Batch Size: 64'\n","Success: Plugin: gemini with Batch Size: 64\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ echo 'Running with plugin: low_level_zero and batch size: 64'\n","Running with plugin: low_level_zero and batch size: 64\n","+ torchrun --standalone --nproc_per_node=1 finetune.py --target_f1 0.86 --plugin low_level_zero --batch_size 64\n","2024-04-12 03:34:17.296756: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-12 03:34:17.296804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-12 03:34:17.298219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-12 03:34:18.386269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 03:34:21]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","Map: 100% 3668/3668 [00:00<00:00, 8633.77 examples/s]\n","Map: 100% 408/408 [00:00<00:00, 8890.07 examples/s]\n","Map: 100% 1725/1725 [00:00<00:00, 9557.48 examples/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0926060676574707 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10886144638061523 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","Epoch [1/3]:   0% 0/57 [00:01<?, ?it/s, loss=0.643]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 57/57 [00:21<00:00,  2.69it/s, loss=0.554]\n","Epoch [2/3]: 100% 57/57 [00:21<00:00,  2.70it/s, loss=0.34]\n","Epoch [3/3]: 100% 57/57 [00:21<00:00,  2.70it/s, loss=0.28]\n","{'accuracy': 0.8231884057971014, 'f1': 0.8721174004192872, 'loss': 0.41884133219718933}\n","+ '[' 0 -eq 0 ']'\n","+ echo 'Success: Plugin: low_level_zero with Batch Size: 64'\n","Success: Plugin: low_level_zero with Batch Size: 64\n"]}],"source":["!bash test_ci.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":668619,"status":"ok","timestamp":1712904926996,"user":{"displayName":"jingjing sun","userId":"10008707095167667488"},"user_tz":-480},"id":"pQO913AACPNz","outputId":"7e299126-d01a-4eb5-d258-9422d51b0f7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ torchrun --standalone --nproc_per_node 1 benchmark.py --plugin torch_ddp --model_type bert\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 06:44:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09444022178649902 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10942697525024414 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 63/63 [01:34<00:00,  1.50s/it]\n","Epoch [2/3]: 100% 63/63 [01:46<00:00,  1.70s/it]\n","Epoch [3/3]: 100% 63/63 [01:47<00:00,  1.70s/it]\n","{'params': '86.82 M', 'throughput': {'batch_size_16': {'throughput:': '0.6'}}, 'memory': {'batch_size_16': {'cuda_pre_training_bytes': '1.31 GB', 'cuda_max_training_bytes': '11.19 GB', 'cuda_post_training_bytes': '1.31 GB'}}}\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ torchrun --standalone --nproc_per_node 1 benchmark.py --plugin torch_ddp_fp16 --model_type bert\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 06:49:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09312295913696289 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10913705825805664 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 63/63 [00:39<00:00,  1.60it/s]\n","Epoch [2/3]: 100% 63/63 [00:38<00:00,  1.62it/s]\n","Epoch [3/3]: 100% 63/63 [00:39<00:00,  1.61it/s]\n","{'params': '86.82 M', 'throughput': {'batch_size_16': {'throughput:': '1.6'}}, 'memory': {'batch_size_16': {'cuda_pre_training_bytes': '1.32 GB', 'cuda_max_training_bytes': '8.72 GB', 'cuda_post_training_bytes': '1.32 GB'}}}\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ torchrun --standalone --nproc_per_node 1 benchmark.py --plugin gemini --model_type bert\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 06:52:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0924530029296875 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10825657844543457 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","Epoch [1/3]: 100% 63/63 [00:28<00:00,  2.17it/s]\n","Epoch [2/3]: 100% 63/63 [00:29<00:00,  2.14it/s]\n","Epoch [3/3]: 100% 63/63 [00:28<00:00,  2.24it/s]\n","{'params': '86.82 M', 'throughput': {'batch_size_16': {'throughput:': '2.2'}}, 'memory': {'batch_size_16': {'cuda_pre_training_bytes': '1.19 GB', 'cuda_max_training_bytes': '6.49 GB', 'cuda_post_training_bytes': '1.19 GB'}}}\n","+ for plugin in \"torch_ddp\" \"torch_ddp_fp16\" \"gemini\" \"low_level_zero\"\n","+ torchrun --standalone --nproc_per_node 1 benchmark.py --plugin low_level_zero --model_type bert\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","\u001b[2;36m[04/12/24 06:53:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                        \n","\u001b[2;36m                    \u001b[0m         \u001b[35m/usr/local/lib/python3.10/dist-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m67\u001b[0m    \n","\u001b[2;36m                    \u001b[0m         launch                                                                 \n","\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized,\n","\u001b[2;36m                    \u001b[0m         world size: \u001b[1;36m1\u001b[0m                                                          \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0989842414855957 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11793303489685059 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Epoch [1/3]: 100% 63/63 [00:31<00:00,  2.00it/s]\n","Epoch [2/3]: 100% 63/63 [00:31<00:00,  1.98it/s]\n","Epoch [3/3]: 100% 63/63 [00:31<00:00,  2.03it/s]\n","{'params': '86.82 M', 'throughput': {'batch_size_16': {'throughput:': '2.0'}}, 'memory': {'batch_size_16': {'cuda_pre_training_bytes': '1.20 GB', 'cuda_max_training_bytes': '6.50 GB', 'cuda_post_training_bytes': '1.20 GB'}}}\n"]}],"source":["!bash benchmark.sh"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN/ptPNS4unxLWqXJrwgUC7","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
